{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('miss').getOrCreate()\nfile_path = '/FileStore/tables/ContainsNull_1_-020da.csv'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["df = spark.read.csv(file_path, inferSchema=True,header=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John| null|\nemp2| null| null|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Drop any row that contains missing data\n#Drop the missing data\n#You can use the .na functions for missing data. The drop command has the following parameters:\n#\n#df.na.drop(how='any', thresh=None, subset=None)\n#\n#* param how: 'any' or 'all'.\n#\n#   If 'any', drop a row if it contains any nulls.\n#    If 'all', drop a row only if all its values are null.\n#\n#* param thresh: int, default None\n#\n#    If specified, drop rows that have less than `thresh` non-null values.\n#    This overwrites the `how` parameter.\n#\n#* param subset: \n#    optional list of column names to consider.\ndf.na.drop().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Has to have at least 2 NON-null values\ndf.na.drop(how='all', thresh=2).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John| null|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Drop null for Named columns \ndf.na.drop(subset=[\"Name\"]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John| null|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["#how If 'any', drop a row if it contains any nulls.\ndf.na.drop(how='any').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# If 'all', drop a row only if all its values are null.\ndf.na.drop(how='all').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John| null|\nemp2| null| null|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["#Fill the missing values\n#We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up #the data types. For example:\ndf.na.fill('NEW VALUES').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----------+-----+\n  Id|      Name|Sales|\n+----+----------+-----+\nemp1|      John| null|\nemp2|NEW VALUES| null|\nemp3|NEW VALUES|345.0|\nemp4|     Cindy|456.0|\n+----+----------+-----+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["df.na.fill(0).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John|  0.0|\nemp2| null|  0.0|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Usually you should specify what columns you want to fill with the subset parameter\ndf.na.fill('No Names', subset=['Name']).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------+-----+\n  Id|    Name|Sales|\n+----+--------+-----+\nemp1|    John| null|\nemp2|No Names| null|\nemp3|No Names|345.0|\nemp4|   Cindy|456.0|\n+----+--------+-----+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["help(df.na.fill)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Help on method fill in module pyspark.sql.dataframe:\n\nfill(value, subset=None) method of pyspark.sql.dataframe.DataFrameNaFunctions instance\n    Replace null values, alias for ``na.fill()``.\n    :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n    \n    :param value: int, long, float, string, bool or dict.\n        Value to replace null values with.\n        If the value is a dict, then `subset` is ignored and `value` must be a mapping\n        from column name (string) to replacement value. The replacement value must be\n        an int, long, float, boolean, or string.\n    :param subset: optional list of column names to consider.\n        Columns specified in subset that do not have matching data type are ignored.\n        For example, if `value` is a string, and subset contains a non-string column,\n        then the non-string column is simply ignored.\n    \n    &gt;&gt;&gt; df4.na.fill(50).show()\n    +---+------+-----+\nage|height| name|\n    +---+------+-----+\n 10|    80|Alice|\n  5|    50|  Bob|\n 50|    50|  Tom|\n 50|    50| null|\n    +---+------+-----+\n    \n    &gt;&gt;&gt; df5.na.fill(False).show()\n    +----+-------+-----+\n age|   name|  spy|\n    +----+-------+-----+\n  10|  Alice|false|\n   5|    Bob|false|\nnull|Mallory| true|\n    +----+-------+-----+\n    \n    &gt;&gt;&gt; df4.na.fill({&#39;age&#39;: 50, &#39;name&#39;: &#39;unknown&#39;}).show()\n    +---+------+-------+\nage|height|   name|\n    +---+------+-------+\n 10|    80|  Alice|\n  5|  null|    Bob|\n 50|  null|    Tom|\n 50|  null|unknown|\n    +---+------+-------+\n    \n    .. versionadded:: 1.3.1\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["df.na.fill({\"Name\":\"No Name\",\"sales\":0}).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-------+-----+\n  Id|   Name|Sales|\n+----+-------+-----+\nemp1|   John|  0.0|\nemp2|No Name|  0.0|\nemp3|No Name|345.0|\nemp4|  Cindy|456.0|\n+----+-------+-----+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["# A very common practice is to fill values with the mean value for the column, for example:\nfrom pyspark.sql.functions import mean\nmean_val = df.select(mean(df['Sales'])).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["mean_sales=mean_val[0][0]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["df.na.fill(mean_sales,[\"Sales\"]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\n  Id| Name|Sales|\n+----+-----+-----+\nemp1| John|400.5|\nemp2| null|400.5|\nemp3| null|345.0|\nemp4|Cindy|456.0|\n+----+-----+-----+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"PySpark_Null_Values","notebookId":1505906375762615},"nbformat":4,"nbformat_minor":0}
